{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2df9be-6d76-409c-bf6b-817945c80e82",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6662a6-18c1-481a-ba76-388a0280842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b979e-e3c1-4f75-be82-0ce3ba44bdda",
   "metadata": {},
   "source": [
    "### Load and merge raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d1c13a-52ba-4f14-a036-4c8d0780826e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review file processed.\n",
      "Unique gmap_ids: 513134\n",
      "\n",
      "Meta file processed.\n",
      "\n",
      "*Combined Dataset*\n",
      "Total size: (70632313, 9)\n",
      "Total reviews: 70632313\n",
      "\n",
      "Saving in chunks...\n",
      "Saved rows 0 to 1,000,000\n",
      "Saved rows 1,000,000 to 2,000,000\n",
      "Saved rows 2,000,000 to 3,000,000\n",
      "Saved rows 3,000,000 to 4,000,000\n",
      "Saved rows 4,000,000 to 5,000,000\n",
      "Saved rows 5,000,000 to 6,000,000\n",
      "Saved rows 6,000,000 to 7,000,000\n",
      "Saved rows 7,000,000 to 8,000,000\n",
      "Saved rows 8,000,000 to 9,000,000\n",
      "Saved rows 9,000,000 to 10,000,000\n",
      "Saved rows 10,000,000 to 11,000,000\n",
      "Saved rows 11,000,000 to 12,000,000\n",
      "Saved rows 12,000,000 to 13,000,000\n",
      "Saved rows 13,000,000 to 14,000,000\n",
      "Saved rows 14,000,000 to 15,000,000\n",
      "Saved rows 15,000,000 to 16,000,000\n",
      "Saved rows 16,000,000 to 17,000,000\n",
      "Saved rows 17,000,000 to 18,000,000\n",
      "Saved rows 18,000,000 to 19,000,000\n",
      "Saved rows 19,000,000 to 20,000,000\n",
      "Saved rows 20,000,000 to 21,000,000\n",
      "Saved rows 21,000,000 to 22,000,000\n",
      "Saved rows 22,000,000 to 23,000,000\n",
      "Saved rows 23,000,000 to 24,000,000\n",
      "Saved rows 24,000,000 to 25,000,000\n",
      "Saved rows 25,000,000 to 26,000,000\n",
      "Saved rows 26,000,000 to 27,000,000\n",
      "Saved rows 27,000,000 to 28,000,000\n",
      "Saved rows 28,000,000 to 29,000,000\n",
      "Saved rows 29,000,000 to 30,000,000\n",
      "Saved rows 30,000,000 to 31,000,000\n",
      "Saved rows 31,000,000 to 32,000,000\n",
      "Saved rows 32,000,000 to 33,000,000\n",
      "Saved rows 33,000,000 to 34,000,000\n",
      "Saved rows 34,000,000 to 35,000,000\n",
      "Saved rows 35,000,000 to 36,000,000\n",
      "Saved rows 36,000,000 to 37,000,000\n",
      "Saved rows 37,000,000 to 38,000,000\n",
      "Saved rows 38,000,000 to 39,000,000\n",
      "Saved rows 39,000,000 to 40,000,000\n",
      "Saved rows 40,000,000 to 41,000,000\n",
      "Saved rows 41,000,000 to 42,000,000\n",
      "Saved rows 42,000,000 to 43,000,000\n",
      "Saved rows 43,000,000 to 44,000,000\n",
      "Saved rows 44,000,000 to 45,000,000\n",
      "Saved rows 45,000,000 to 46,000,000\n",
      "Saved rows 46,000,000 to 47,000,000\n",
      "Saved rows 47,000,000 to 48,000,000\n",
      "Saved rows 48,000,000 to 49,000,000\n",
      "Saved rows 49,000,000 to 50,000,000\n",
      "Saved rows 50,000,000 to 51,000,000\n",
      "Saved rows 51,000,000 to 52,000,000\n",
      "Saved rows 52,000,000 to 53,000,000\n",
      "Saved rows 53,000,000 to 54,000,000\n",
      "Saved rows 54,000,000 to 55,000,000\n",
      "Saved rows 55,000,000 to 56,000,000\n",
      "Saved rows 56,000,000 to 57,000,000\n",
      "Saved rows 57,000,000 to 58,000,000\n",
      "Saved rows 58,000,000 to 59,000,000\n",
      "Saved rows 59,000,000 to 60,000,000\n",
      "Saved rows 60,000,000 to 61,000,000\n",
      "Saved rows 61,000,000 to 62,000,000\n",
      "Saved rows 62,000,000 to 63,000,000\n",
      "Saved rows 63,000,000 to 64,000,000\n",
      "Saved rows 64,000,000 to 65,000,000\n",
      "Saved rows 65,000,000 to 66,000,000\n",
      "Saved rows 66,000,000 to 67,000,000\n",
      "Saved rows 67,000,000 to 68,000,000\n",
      "Saved rows 68,000,000 to 69,000,000\n",
      "Saved rows 69,000,000 to 70,000,000\n",
      "Saved rows 70,000,000 to 70,632,313\n",
      "Combined df saved as json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviews json file\n",
    "review_raw = \"data/review-California.json\"\n",
    "review_filtered = \"sampled_data/review_California_filtered.json\"\n",
    "\n",
    "# Filter review data for relevant columns\n",
    "review_cols = ['user_id', 'time', 'rating', 'text', 'gmap_id']\n",
    "gmap_ids = set()\n",
    "\n",
    "with open(review_raw, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(review_filtered, \"w\", encoding=\"utf-8\") as fout:\n",
    "    \n",
    "    for line in fin:\n",
    "        obj = json.loads(line)\n",
    "        # Keep only relevant columns\n",
    "        filtered_obj = {k: obj.get(k) for k in review_cols}\n",
    "        fout.write(json.dumps(filtered_obj) + \"\\n\")\n",
    "        gmap_ids.add(obj[\"gmap_id\"])\n",
    "        del obj, filtered_obj  # Clear memory\n",
    "\n",
    "print(\"Review file processed.\")\n",
    "print(f\"Unique gmap_ids: {len(gmap_ids)}\")\n",
    "print()\n",
    "\n",
    "# Meta json file\n",
    "meta_raw = \"data/meta-California.json\"\n",
    "meta_filtered = \"sampled_data/meta_California_filtered.json\"\n",
    "\n",
    "# Filter meta data for relevant columns\n",
    "meta_cols = ['gmap_id', 'name', 'category', 'avg_rating', 'num_of_reviews']\n",
    "\n",
    "with open(meta_raw, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(meta_filtered, \"w\", encoding=\"utf-8\") as fout:\n",
    "    \n",
    "    for line in fin:\n",
    "        obj = json.loads(line)\n",
    "        if obj[\"gmap_id\"] in gmap_ids:\n",
    "            # Keep only relevant columns\n",
    "            filtered_obj = {k: obj.get(k) for k in meta_cols}\n",
    "            fout.write(json.dumps(filtered_obj) + \"\\n\")\n",
    "            del obj, filtered_obj  # Clear memory\n",
    "\n",
    "# Clear gmap_ids set\n",
    "del gmap_ids\n",
    "gc.collect()\n",
    "\n",
    "print(\"Meta file processed.\")\n",
    "print()\n",
    "\n",
    "# Merge Review & Meta\n",
    "reviews = pd.read_json(review_filtered, lines=True)\n",
    "meta = pd.read_json(meta_filtered, lines=True)\n",
    "\n",
    "# Merge on gmap_id\n",
    "df = reviews.merge(meta, on='gmap_id', how='left', suffixes=('_review', '_business'))\n",
    "\n",
    "# Clear intermediate dataframes\n",
    "del reviews, meta\n",
    "gc.collect()\n",
    "\n",
    "# Rename 'name' to 'name_business' for consistency\n",
    "if 'name' in df.columns:\n",
    "    df.rename(columns={'name': 'name_business'}, inplace=True)\n",
    "\n",
    "print(\"*Combined Dataset*\")\n",
    "print(f\"Total size: {df.shape}\")\n",
    "print(f\"Total reviews: {len(df)}\")\n",
    "print()\n",
    "\n",
    "# Save in chunks to avoid memory crash\n",
    "combined_sample = \"data/combined_California_filtered.json\"\n",
    "chunk_size = 1_000_000  # 1M rows at a time\n",
    "\n",
    "print(\"Saving in chunks...\")\n",
    "for i in range(0, len(df), chunk_size):\n",
    "    chunk = df.iloc[i:i+chunk_size]\n",
    "    mode = 'w' if i == 0 else 'a'  # Write first chunk, append rest\n",
    "    chunk.to_json(combined_sample, orient='records', lines=True, mode=mode)\n",
    "    print(f\"Saved rows {i:,} to {min(i+chunk_size, len(df)):,}\")\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Combined df saved as json.\")\n",
    "\n",
    "# Clear df after saving\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac6ae55-427d-4a7e-909e-f019d7453232",
   "metadata": {},
   "source": [
    "#### This json file was still 25GB once saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb39132-9083-47c0-95bd-32fb999684d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49208a17-4144-4e7a-962c-588f09e186fc",
   "metadata": {},
   "source": [
    "### Load merged data\n",
    "\n",
    "##### * Future processing start from here *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc895d60-3046-44e8-8699-30307b42b48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in chunks...\n",
      "Loaded chunk 10: 1,000,000 rows\n",
      "Loaded chunk 20: 1,000,000 rows\n",
      "Loaded chunk 30: 1,000,000 rows\n",
      "Loaded chunk 40: 1,000,000 rows\n",
      "Loaded chunk 50: 1,000,000 rows\n",
      "Loaded chunk 60: 1,000,000 rows\n",
      "Loaded chunk 70: 1,000,000 rows\n",
      "\n",
      "Total merged rows: 70,632,313\n"
     ]
    }
   ],
   "source": [
    "# LOAD MERGED DATASET\n",
    "\n",
    "chunk_size = 1_000_000  # 1M rows at a time\n",
    "chunks = []\n",
    "\n",
    "print(\"Reading in chunks...\")\n",
    "for i, chunk in enumerate(pd.read_json('data/combined_California_filtered.json', \n",
    "                                        lines=True, \n",
    "                                        chunksize=chunk_size)):\n",
    "    chunks.append(chunk)\n",
    "    if (i + 1) % 10 == 0:  # Print every 10 chunks\n",
    "        print(f\"Loaded chunk {i+1}: {len(chunk):,} rows\")\n",
    "\n",
    "# Combine chunks\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nTotal merged rows: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2662269c-c84f-4a99-9ee1-ccc90483937c",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4dae444-9dac-4803-abe3-e81da638255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Duplicate Review Check*\n",
      "Total rows before: 70,632,313\n",
      "Duplicate reviews found: 976,178\n",
      "Total rows after: 69,656,135\n",
      "Rows after removing NaN ratings: 69,286,010\n",
      "\n",
      "Rows after removing reviews with empty text: 38,074,228\n",
      "\n",
      "<class 'pandas.DataFrame'>\n",
      "Index: 38074228 entries, 0 to 70632058\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   user_id         float64\n",
      " 1   time            int64  \n",
      " 2   rating          float64\n",
      " 3   text            str    \n",
      " 4   gmap_id         str    \n",
      " 5   name_business   str    \n",
      " 6   category        object \n",
      " 7   avg_rating      float64\n",
      " 8   num_of_reviews  int64  \n",
      "dtypes: float64(3), int64(2), object(1), str(3)\n",
      "memory usage: 10.1+ GB\n"
     ]
    }
   ],
   "source": [
    "# Duplicate check and delete\n",
    "print(\"*Duplicate Review Check*\")\n",
    "print(f\"Total rows before: {len(df):,}\")\n",
    "duplicate_mask = df.duplicated(subset=['user_id', 'gmap_id', 'time'], keep='first') # If user, business and time match, consider that a duplicate\n",
    "print(f\"Duplicate reviews found: {duplicate_mask.sum():,}\")\n",
    "df = df[~duplicate_mask].copy()\n",
    "print(f\"Total rows after: {len(df):,}\")\n",
    "del duplicate_mask\n",
    "gc.collect()\n",
    "\n",
    "# Remove ratings with NaN\n",
    "df = df.dropna(subset=['rating'])\n",
    "print(f\"Rows after removing NaN ratings: {len(df):,}\")\n",
    "print()\n",
    "\n",
    "# Remove reviews with NaN or missing text\n",
    "df = df[df['text'].notna() & (df['text'].str.strip() != '')]\n",
    "print(f\"Rows after removing reviews with empty text: {len(df):,}\")\n",
    "print()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530d2731-e103-47f4-8bfa-f33bd529cf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to csv.\n"
     ]
    }
   ],
   "source": [
    "# Temporary saving point for a manageable size file for eda\n",
    "df.to_csv('data/cleaned_for_eda.csv', index=False)\n",
    "print(\"Saved to csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d17ff-ce29-4fc2-b38f-25fc0b1d034d",
   "metadata": {},
   "source": [
    "## More possible filters for modeling  \n",
    "- businesses with more than 10 reviews, user_id with 5 or more reviews (businesses with more than 10 reviews and reviewers with more than 5 reviews are likely to yield more consistent results, less outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29caf3d8-c59f-454a-a3b3-9f9522f3b013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
