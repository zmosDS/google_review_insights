{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6662a6-18c1-481a-ba76-388a0280842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d1c13a-52ba-4f14-a036-4c8d0780826e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "paths = {\n",
    "    'review_raw': \"../data/review-California.json\",\n",
    "    'meta_raw': \"../data/meta-California.json\",\n",
    "    'combined': \"../data/combined_California.json\",\n",
    "    'cleaned_eda': \"../data/cleaned_for_eda.csv\",\n",
    "    'cleaned_model': \"../data/cleaned_for_modeling.csv\"\n",
    "}\n",
    "\n",
    "# Processing chunk size\n",
    "chunk_size = 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b979e-e3c1-4f75-be82-0ce3ba44bdda",
   "metadata": {},
   "source": [
    "# Process raw files pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905cdf2c-ba09-46b6-b065-3ee964322905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing raw review file...\n",
      "Reviews loaded. Unique gmap_ids: 513134\n",
      "Total reviews: 70,529,977\n",
      "\n",
      "Processing raw meta file...\n",
      "Meta loaded. Total businesses: 515,961\n",
      "\n",
      "Merging datasets...\n",
      "*Combined Dataset*\n",
      "Total size: (70632313, 9)\n",
      "Total reviews: 70,632,313\n",
      "\n",
      "Saving in chunks...\n",
      "Saved chunk 10: 10,000,000 total rows\n",
      "Saved chunk 20: 20,000,000 total rows\n",
      "Saved chunk 30: 30,000,000 total rows\n",
      "Saved chunk 40: 40,000,000 total rows\n",
      "Saved chunk 50: 50,000,000 total rows\n",
      "Saved chunk 60: 60,000,000 total rows\n",
      "Saved chunk 70: 70,000,000 total rows\n",
      "Saved chunk 71: 70,632,313 total rows\n",
      "Combined df saved to ../data/combined_California.json\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Relevant columns to filter for\n",
    "review_columns = ['user_id', 'time', 'rating', 'text', 'gmap_id']\n",
    "meta_columns = ['gmap_id', 'name', 'category', 'avg_rating', 'num_of_reviews']\n",
    "\n",
    "# Process reviews and extract gmap_ids\n",
    "print(\"Processing raw review file...\")\n",
    "reviews_list = []\n",
    "gmap_ids = set()\n",
    "\n",
    "with open(paths['review_raw'], \"r\", encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        obj = json.loads(line)\n",
    "        filtered_obj = {k: obj.get(k) for k in review_columns}\n",
    "        reviews_list.append(filtered_obj)\n",
    "        gmap_ids.add(obj[\"gmap_id\"])\n",
    "\n",
    "reviews = pd.DataFrame(reviews_list)\n",
    "del reviews_list\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Reviews loaded. Unique gmap_ids: {len(gmap_ids)}\")\n",
    "print(f\"Total reviews: {len(reviews):,}\\n\")\n",
    "\n",
    "# Filter meta data\n",
    "print(\"Processing raw meta file...\")\n",
    "meta_list = []\n",
    "\n",
    "with open(paths['meta_raw'], \"r\", encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        obj = json.loads(line)\n",
    "        if obj[\"gmap_id\"] in gmap_ids:\n",
    "            filtered_obj = {k: obj.get(k) for k in meta_columns}\n",
    "            meta_list.append(filtered_obj)\n",
    "\n",
    "meta = pd.DataFrame(meta_list)\n",
    "del meta_list, gmap_ids\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Meta loaded. Total businesses: {len(meta):,}\\n\")\n",
    "\n",
    "# Merge datasets on gmap_id\n",
    "print(\"Merging datasets...\")\n",
    "df = reviews.merge(meta, on='gmap_id', how='left', suffixes=('_review', '_business'))\n",
    "\n",
    "del reviews, meta\n",
    "gc.collect()\n",
    "\n",
    "# Rename business name column for clarity\n",
    "if 'name' in df.columns:\n",
    "    df = df.rename(columns={'name': 'name_business'})\n",
    "\n",
    "print(\"*Combined Dataset*\")\n",
    "print(f\"Total size: {df.shape}\")\n",
    "print(f\"Total reviews: {len(df):,}\\n\")\n",
    "\n",
    "# Function to save data in chunks\n",
    "def save_in_chunks(df, output_path, chunk_size=chunk_size):\n",
    "    \"\"\"Save dataframe in chunks to avoid memory issues.\"\"\"\n",
    "    print(\"Saving in chunks...\")\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        mode = 'w' if i == 0 else 'a'\n",
    "        chunk.to_json(output_path, orient='records', lines=True, mode=mode)\n",
    "        \n",
    "        chunk_num = i // chunk_size + 1\n",
    "        if chunk_num % 10 == 0 or i + chunk_size >= len(df):\n",
    "            total_saved = min(i + chunk_size, len(df))\n",
    "            print(f\"Saved chunk {chunk_num}: {total_saved:,} total rows\")\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    print(f\"Combined df saved to {output_path}\")\n",
    "\n",
    "# Save merged data\n",
    "save_in_chunks(df, paths['combined'])\n",
    "\n",
    "del df\n",
    "gc.collect()\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac6ae55-427d-4a7e-909e-f019d7453232",
   "metadata": {},
   "source": [
    "## **Column descriptions**\n",
    "**Review File**  \n",
    "**user_id** - ID of the reviewer  \n",
    "**name** - name of the reviwer  \n",
    "**time** - time of the review (unix time)  \n",
    "**rating** - rating of the business  \n",
    "**text** - text of the review  \n",
    "**pics** - pictures of the review  \n",
    "**resp** - business response to the review including unix time and text of the response  \n",
    "**Meta File**  \n",
    "**gmap_id** - ID of the business  \n",
    "**name** - name of the business  \n",
    "**address** - address of the business  \n",
    "**gmap_id** - ID of the business  \n",
    "**description** - description of the business  \n",
    "**latitude** - latitude of the business  \n",
    "**longitude** - longitude of the business  \n",
    "**category** - category of the business   \n",
    "**avg_rating** - average rating of the business  \n",
    "**num_of_reviews** - number of reviews  \n",
    "**price** - price of the business  \n",
    "**hours** - open hours  \n",
    "**MISC** - MISC information  \n",
    "**state** - the current status of the business (e.g., permanently closed)  \n",
    "**relative_results** - relative businesses recommended by Google  \n",
    "**url** - URL of the business\n",
    "<br>\n",
    "<br>\n",
    "### **Filtered these columns out**  \n",
    "**'name_review'** - Reviewer name (can use id instead)  \n",
    "**'pics'** - don't need  \n",
    "**'resp'** - unlikely to be relevant could bring back   \n",
    "**'address'** - remove for now, might extract city, state at some point  \n",
    "**'description'** - mostly empty values  \n",
    "**'latitude' & 'longitude'** - don't need  \n",
    "**'price', 'hours', 'MISC', 'state', 'relative_results', 'url'** - not relevant  \n",
    "\n",
    "### After merging and filtering the json was over 25GB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb39132-9083-47c0-95bd-32fb999684d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2662269c-c84f-4a99-9ee1-ccc90483937c",
   "metadata": {},
   "source": [
    "# Clean data for EDA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc895d60-3046-44e8-8699-30307b42b48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in chunks...\n",
      "Loaded chunk 10: 10,000,000 total rows\n",
      "Loaded chunk 20: 20,000,000 total rows\n",
      "Loaded chunk 30: 30,000,000 total rows\n",
      "Loaded chunk 40: 40,000,000 total rows\n",
      "Loaded chunk 50: 50,000,000 total rows\n",
      "Loaded chunk 60: 60,000,000 total rows\n",
      "Loaded chunk 70: 70,000,000 total rows\n",
      "\n",
      "Total merged rows: 70,632,313\n"
     ]
    }
   ],
   "source": [
    "# CLEANING FOR EDA CAN START HERE\n",
    "# Run cells 1 & 2 first\n",
    "# Then run this cell to load merged json data into df\n",
    "\n",
    "# Load merged json data in chunks\n",
    "chunks = []\n",
    "print(\"Reading in chunks...\")\n",
    "for i, chunk in enumerate(pd.read_json(paths['combined'], \n",
    "                                        lines=True, \n",
    "                                        chunksize=chunk_size)):\n",
    "    chunks.append(chunk)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        total_rows = (i + 1) * chunk_size\n",
    "        print(f\"Loaded chunk {i+1}: {total_rows:,} total rows\")\n",
    "\n",
    "# Combine chunks to dataframe\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nTotal merged rows: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4dae444-9dac-4803-abe3-e81da638255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicates: 70,632,313 rows\n",
      "  → 69,656,135 rows (976,178 removed)\n",
      "\n",
      "Removing missing ratings: 69,656,135 rows\n",
      "  → 69,286,010 rows (370,125 removed)\n",
      "\n",
      "Removing empty text: 69,286,010 rows\n",
      "  → 38,074,228 rows (31,211,782 removed)\n",
      "\n",
      "Saved to ../data/cleaned_for_eda.csv\n",
      "\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 38074228 entries, 0 to 38074227\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   user_id         float64\n",
      " 1   time            int64  \n",
      " 2   rating          float64\n",
      " 3   text            str    \n",
      " 4   gmap_id         str    \n",
      " 5   name_business   str    \n",
      " 6   category        object \n",
      " 7   avg_rating      float64\n",
      " 8   num_of_reviews  int64  \n",
      "dtypes: float64(3), int64(2), object(1), str(3)\n",
      "memory usage: 9.8+ GB\n"
     ]
    }
   ],
   "source": [
    "# Basic functions to prepare data for eda\n",
    "# Remove duplicates, missing ratings, missing(empty) reviews\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    \"\"\"Remove duplicate reviews based on user_id, gmap_id, and time.\"\"\"\n",
    "    print(f\"Removing duplicates: {len(df):,} rows\")\n",
    "    duplicate_mask = df.duplicated(subset=['user_id', 'gmap_id', 'time'], keep='first')\n",
    "    df_clean = df[~duplicate_mask].copy()\n",
    "    print(f\"  → {len(df_clean):,} rows ({duplicate_mask.sum():,} removed)\\n\")\n",
    "    del duplicate_mask\n",
    "    gc.collect()\n",
    "    return df_clean\n",
    "\n",
    "def remove_missing_ratings(df):\n",
    "    \"\"\"Remove rows with NaN ratings.\"\"\"\n",
    "    print(f\"Removing missing ratings: {len(df):,} rows\")\n",
    "    df_clean = df.dropna(subset=['rating'])\n",
    "    print(f\"  → {len(df_clean):,} rows ({len(df) - len(df_clean):,} removed)\\n\")\n",
    "    return df_clean\n",
    "\n",
    "def remove_empty_text(df):\n",
    "    \"\"\"Remove reviews with NaN or empty text.\"\"\"\n",
    "    print(f\"Removing empty text: {len(df):,} rows\")\n",
    "    df_clean = df[df['text'].notna() & (df['text'].str.strip() != '')]\n",
    "    print(f\"  → {len(df_clean):,} rows ({len(df) - len(df_clean):,} removed)\\n\")\n",
    "    return df_clean\n",
    "\n",
    "# Apply eda cleaning pipeline\n",
    "df = (df\n",
    "    .pipe(remove_duplicates)\n",
    "    .pipe(remove_missing_ratings)\n",
    "    .pipe(remove_empty_text)\n",
    "    .reset_index(drop=True) # reset index\n",
    ")\n",
    "\n",
    "# Save eda cleaned data\n",
    "df.to_csv(paths['cleaned_eda'], index=False)\n",
    "print(f\"Saved to {paths['cleaned_eda']}\")\n",
    "print()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7cc83-1e91-40d5-8663-a156fae01f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d9d0524-3d0e-4bd7-bc06-1874559b51c0",
   "metadata": {},
   "source": [
    "# Clean data for modeling pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ebcd0-aad3-41a8-8c60-9773a7ec80e3",
   "metadata": {},
   "source": [
    "### Model filtering based on EDA\n",
    "- Filter columns: ['user_id', 'rating', 'text', 'gmap_id', 'time']\n",
    "  * rating & text are required for ABSA modeling\n",
    "  * user_id & gmap_id needed for filtering, then dropped\n",
    "  * time needed for temporal filtering\n",
    "  * category has 86,000+ unique values, requires extensive cleaning\n",
    "  * name_business, avg_rating, num_of_reviews not relevant to modeling\n",
    "- Filter for users with 5-250 reviews (remove sparse and spam users)\n",
    "- Filter for businesses with 10+ reviews (remove unreliable businesses)\n",
    "- Filter for reviews from 2011+ (10-year recency, pre-2010 is 0.02% of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c522222-3aaa-4612-99b5-3de7fb164ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned EDA dataset...\n",
      "Loaded 38,074,228 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CLEANING FOR MODEL CAN START HERE\n",
    "# Run cells 1 & 2 first\n",
    "# Then run this cell to cleaned for eda dataset into df\n",
    "\n",
    "# Load cleaned dataset\n",
    "print(\"Loading cleaned EDA dataset...\")\n",
    "df = pd.read_csv(paths['cleaned_eda'])\n",
    "print(f\"Loaded {len(df):,} rows\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9284456-602f-4926-9237-af99c96ac05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering years (2011+): 38,074,228 rows\n",
      "  After: 38,053,909 rows (20,319 removed)\n",
      "\n",
      "Selecting model columns: 38,053,909 rows\n",
      "  After: 38,053,909 rows (kept 4 columns)\n",
      "\n",
      "Filtering users (5-250 reviews): 38,053,909 rows\n",
      "  After: 25,077,508 rows (12,976,401 removed)\n",
      "\n",
      "Filtering businesses (10+ reviews): 25,077,508 rows\n",
      "  After: 24,172,346 rows (905,162 removed)\n",
      "\n",
      "Saved to ../data/cleaned_for_modeling.csv\n",
      "\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 24172346 entries, 0 to 24172345\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   rating   float64\n",
      " 2   text     str    \n",
      " 3   gmap_id  str    \n",
      "dtypes: float64(2), str(2)\n",
      "memory usage: 4.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Functions to clean data for modeling\n",
    "\n",
    "def select_model_columns(df):\n",
    "    \"\"\"Select only columns needed for modeling.\"\"\"\n",
    "    print(f\"Selecting model columns: {len(df):,} rows\")\n",
    "    model_cols = ['user_id', 'rating', 'text', 'gmap_id']\n",
    "    df_clean = df[model_cols].copy()\n",
    "    print(f\"  After: {len(df_clean):,} rows (kept {len(model_cols)} columns)\\n\")\n",
    "    return df_clean\n",
    "\n",
    "def filter_user_review_range(df, min_reviews=5, max_reviews=250):\n",
    "    \"\"\"Filter for users with review count between min and max.\"\"\"\n",
    "    print(f\"Filtering users ({min_reviews}-{max_reviews} reviews): {len(df):,} rows\")\n",
    "    user_counts = df['user_id'].value_counts()\n",
    "    valid_users = user_counts[(user_counts >= min_reviews) & (user_counts <= max_reviews)].index\n",
    "    df_clean = df[df['user_id'].isin(valid_users)].copy()\n",
    "    print(f\"  After: {len(df_clean):,} rows ({len(df) - len(df_clean):,} removed)\\n\")\n",
    "    return df_clean\n",
    "\n",
    "def filter_business_min_reviews(df, min_reviews=10):\n",
    "    \"\"\"Filter for businesses with minimum review count.\"\"\"\n",
    "    print(f\"Filtering businesses ({min_reviews}+ reviews): {len(df):,} rows\")\n",
    "    business_counts = df['gmap_id'].value_counts()\n",
    "    valid_businesses = business_counts[business_counts >= min_reviews].index\n",
    "    df_clean = df[df['gmap_id'].isin(valid_businesses)].copy()\n",
    "    print(f\"  After: {len(df_clean):,} rows ({len(df) - len(df_clean):,} removed)\\n\")\n",
    "    return df_clean\n",
    "\n",
    "def filter_year_range(df, min_year=2011):\n",
    "    \"\"\"Filter for reviews from min_year onwards.\"\"\"\n",
    "    print(f\"Filtering years ({min_year}+): {len(df):,} rows\")\n",
    "    df['year'] = pd.to_datetime(df['time'], unit='ms').dt.year\n",
    "    df_clean = df[df['year'] >= min_year].copy()\n",
    "    df_clean = df_clean.drop('year', axis=1)\n",
    "    print(f\"  After: {len(df_clean):,} rows ({len(df) - len(df_clean):,} removed)\\n\")\n",
    "    return df_clean\n",
    "\n",
    "# Apply model cleaning pipeline\n",
    "df_model = (df\n",
    "    .pipe(filter_year_range, min_year=2011)\n",
    "    .pipe(select_model_columns)\n",
    "    .pipe(filter_user_review_range, min_reviews=5, max_reviews=250)\n",
    "    .pipe(filter_business_min_reviews, min_reviews=10)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Save model-ready data\n",
    "df_model.to_csv(paths['cleaned_model'], index=False)\n",
    "print(f\"Saved to {paths['cleaned_model']}\\n\")\n",
    "df_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b54d5-30fa-4484-ab30-dad15c89fb4b",
   "metadata": {},
   "source": [
    "### Filtering Decisions for Modeling\n",
    "\n",
    "#### 1. COLUMN SELECTION\n",
    "- **Rationale:** Retained only `user_id`, `rating`, `text`, and `gmap_id` for modeling. `rating` and `text` are the core inputs and labels for ABSA. `user_id` and `gmap_id` are kept only for filtering purposes and will be dropped prior to training. `category`, `avg_rating`, `num_of_reviews`, `name_business`, and `time` were excluded as they are not direct inputs to text-based sentiment modeling.\n",
    "\n",
    "#### 2. USER FILTERING (5-250 reviews)\n",
    "- **EDA Finding:** 50% of users have only 1 review; median is 1 review per user\n",
    "- **Rationale:** Single-review users likely represent one-off experiences that don't reflect consistent reviewer behavior, introducing noise into training data. A minimum of 5 reviews ensures users have enough history to reflect stable preferences. Users with 250+ reviews (0.25% of users) risk being spam accounts or automated reviewers, which could bias the model.\n",
    "- **Impact:** Retains 68% of reviews (25.9M) from 14.5% of users who are consistent reviewers\n",
    "\n",
    "#### 3. BUSINESS FILTERING (10+ reviews)\n",
    "- **EDA Finding:** 34% of businesses have <10 reviews, median is 20 reviews\n",
    "- **Rationale:** Low-review businesses likely lack statistical reliability for modeling. A business with 1-2 reviews doesn't provide enough signal for ABSA training.\n",
    "- **Impact:** Retains 98% of reviews (37.3M), removes 34% of businesses with sparse data\n",
    "\n",
    "#### 4. TEMPORAL FILTERING (2011+)\n",
    "- **EDA Finding:** Reviews grew 5x from 2015-2016, 93% of data is 2016-2021\n",
    "- **Rationale:** Google Reviews launched in 2007, added image support in 2016. Pre-2010 reviews are 0.02% of data and represent outdated platform behavior. 2011 cutoff gives us 10 years of modern review data.\n",
    "- **Impact:** Removes sparse early years, focuses on consistent review platform era\n",
    "\n",
    "#### 5. NO TEXT LENGTH FILTERING\n",
    "- **EDA Finding:** Only 1.1% of reviews exceed 1000 chars, max is 13,057 chars\n",
    "- **Rationale:** Longer reviews often contain more detailed sentiment and aspects. \n",
    "- **Impact:** Preserves all text data, no reviews removed by length\n",
    "\n",
    "### Combined Impact\n",
    "- **Original:** 38.1M reviews\n",
    "- **After filtering:** ~24.1M reviews (63.5% retained)\n",
    "- **Quality improvement:** More consistent users, reliable businesses, modern/recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d09e8-434d-488b-84b8-718167920876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
