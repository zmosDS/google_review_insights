{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad90a916-375c-4b47-a32d-49585abefdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a25d24-ee9e-497a-b78e-beaa0b3ed860",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ede892-90bd-405d-8101-a45391113f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24,188,451 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv('../data/cleaned_for_modeling.csv')\n",
    "print(f\"Loaded {len(df):,} rows\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112ccd1-24c8-49c5-9d5f-89836a6db111",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e4ff7c-abdc-4511-9967-42a37b401d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "def is_likely_english(text):\n",
    "    \"\"\"Fast heuristic: >80% ASCII characters = likely English.\"\"\"\n",
    "    if pd.isna(text) or len(str(text).strip()) == 0:\n",
    "        return False\n",
    "    text = str(text)\n",
    "    ascii_count = sum(1 for c in text if ord(c) < 128)\n",
    "    return (ascii_count / len(text)) > 0.8\n",
    "\n",
    "def filter_english_only(df):\n",
    "    \"\"\"Filter to English-only reviews.\"\"\"\n",
    "    print(f\"Before English filter: {len(df):,} rows\")\n",
    "    df_clean = df.copy()\n",
    "    df_clean['is_english'] = df_clean['text'].apply(is_likely_english)\n",
    "    df_clean = df_clean[df_clean['is_english']].drop(columns=['is_english'])\n",
    "    print(f\"  After: {len(df_clean):,} rows ({len(df) - len(df_clean):,} removed)\\n\")\n",
    "    return df_clean\n",
    "\n",
    "def filter_original_tag(df):\n",
    "    \"\"\"Remove reviews with Google's (Original) translation tag.\"\"\"\n",
    "    print(f\"Before (Original) tag filter: {len(df):,} rows\")\n",
    "    df_clean = df[~df['text'].str.contains(r'\\(Original\\)', na=False)].copy()\n",
    "    print(f\"  After: {len(df_clean):,} rows ({len(df) - len(df_clean):,} removed)\\n\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c9b67d-cd6b-4315-bf84-a97cf6fe5a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before English filter: 24,188,451 rows\n",
      "  After: 24,075,251 rows (113,200 removed)\n",
      "\n",
      "Before (Original) tag filter: 24,075,251 rows\n",
      "  After: 22,640,876 rows (1,434,375 removed)\n",
      "\n",
      "Final dataset: 22,640,876 rows\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 22640876 entries, 0 to 22640875\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   rating   float64\n",
      " 2   text     str    \n",
      " 3   gmap_id  str    \n",
      "dtypes: float64(2), str(2)\n",
      "memory usage: 4.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Apply english filtering pipeline\n",
    "\n",
    "df = (df\n",
    "    .pipe(filter_english_only)\n",
    "    .pipe(filter_original_tag)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Final dataset: {len(df):,} rows\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5303d889-4810-4c3a-885b-085f342f00c1",
   "metadata": {},
   "source": [
    "### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf22bbd-59ee-470a-8cf8-30040e6234b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stratified sample of 250,000 reviews...\n",
      "\n",
      "Rating distribution:\n",
      "rating\n",
      "1.0     15322\n",
      "2.0      9417\n",
      "3.0     21516\n",
      "4.0     50368\n",
      "5.0    153377\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "rating\n",
      "1.0     6.13\n",
      "2.0     3.77\n",
      "3.0     8.61\n",
      "4.0    20.15\n",
      "5.0    61.35\n",
      "Name: proportion, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to sample\n",
    "\n",
    "def create_stratified_sample(df, sample_size=250000, random_state=22):  # 250K sample (adjust size as needed)\n",
    "    \"\"\"Create stratified sample maintaining rating distribution.\"\"\"\n",
    "    print(f\"Creating stratified sample of {sample_size:,} reviews...\")\n",
    "    \n",
    "    df_sample, _ = train_test_split(\n",
    "        df, \n",
    "        train_size=sample_size, \n",
    "        stratify=df['rating'], \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRating distribution:\")\n",
    "    print(df_sample['rating'].value_counts().sort_index())\n",
    "    print(\"\\nPercentages:\")\n",
    "    print((df_sample['rating'].value_counts(normalize=True).sort_index() * 100).round(2))\n",
    "    print()\n",
    "    \n",
    "    return df_sample\n",
    "\n",
    "# Create sample\n",
    "df_250K = create_stratified_sample(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90723ea6-c0b8-4ca1-b656-672c9d1b680f",
   "metadata": {},
   "source": [
    "### Aspect keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b025e1-b278-4374-a98b-bc14a3d12e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect keywords defined:\n",
      "  service: 15 keywords\n",
      "  quality: 15 keywords\n",
      "  cleanliness: 12 keywords\n",
      "  value: 10 keywords\n",
      "  wait_time: 10 keywords\n"
     ]
    }
   ],
   "source": [
    "# 5 basic aspects to get model going\n",
    "# We need to nail down our aspect list\n",
    "\n",
    "aspect_keywords = {\n",
    "    'service': [\n",
    "        'staff', 'waiter', 'waitress', 'server', 'employee', 'service', \n",
    "        'manager', 'cashier', 'worker', 'attendant', 'helpful', 'rude', \n",
    "        'friendly', 'slow service', 'customer service'\n",
    "    ],\n",
    "    'quality': [\n",
    "        'food', 'quality', 'taste', 'delicious', 'fresh', 'stale', 'bland', \n",
    "        'flavor', 'product', 'meal', 'dish', 'overcooked', 'undercooked', \n",
    "        'cold', 'burnt'\n",
    "    ],\n",
    "    'cleanliness': [\n",
    "        'clean', 'dirty', 'filthy', 'hygiene', 'sanitary', 'bathroom', \n",
    "        'restroom', 'table', 'floor', 'messy', 'spotless', 'gross'\n",
    "    ],\n",
    "    'value': [\n",
    "        'price', 'expensive', 'cheap', 'overpriced', 'affordable', 'value', \n",
    "        'worth', 'cost', 'money', 'budget'\n",
    "    ],\n",
    "    'wait_time': [\n",
    "        'wait', 'waiting', 'slow', 'fast', 'quick', 'long wait', 'forever', \n",
    "        'time', 'delayed', 'prompt'\n",
    "    ]\n",
    "}\n",
    "\n",
    "aspects = list(aspect_keywords.keys())\n",
    "sentiments = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Aspect keywords defined:\")\n",
    "for aspect, keywords in aspect_keywords.items():\n",
    "    print(f\"  {aspect}: {len(keywords)} keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c0ab776-7297-4b93-9335-13806172b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to extract aspects from text\n",
    "\n",
    "def extract_aspect_labels(text, rating):\n",
    "    \"\"\"\n",
    "    Extract aspects mentioned in text and assign sentiment based on rating.\n",
    "    \n",
    "    Args:\n",
    "        text: Review text\n",
    "        rating: Overall rating (1-5)\n",
    "        \n",
    "    Returns:\n",
    "        dict: {aspect: sentiment} for aspects found in text\n",
    "    \"\"\"\n",
    "    text_lower = str(text).lower()\n",
    "    aspect_dict = {}\n",
    "    \n",
    "    for aspect, keywords in aspect_keywords.items():\n",
    "        if any(keyword in text_lower for keyword in keywords):\n",
    "            if rating <= 2:\n",
    "                aspect_dict[aspect] = 'negative'\n",
    "            elif rating >= 4:\n",
    "                aspect_dict[aspect] = 'positive'\n",
    "            else:\n",
    "                aspect_dict[aspect] = 'neutral'\n",
    "    \n",
    "    return aspect_dict\n",
    "\n",
    "def clean_text(df):\n",
    "    \"\"\"Clean text column.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    df_clean['text'] = df_clean['text'].fillna('').astype(str)\n",
    "    return df_clean\n",
    "\n",
    "def apply_aspect_extraction(df):\n",
    "    \"\"\"Extract aspect labels from reviews.\"\"\"\n",
    "    print(\"Extracting aspect labels...\")\n",
    "    df_labeled = df.copy()\n",
    "    \n",
    "    df_labeled['aspect_labels'] = df_labeled.apply(\n",
    "        lambda row: extract_aspect_labels(row['text'], row['rating']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"  Processed {len(df_labeled):,} reviews\\n\")\n",
    "    return df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1df3e28-6974-41c1-8deb-6dc05773ccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting aspect labels...\n",
      "  Processed 250,000 reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply aspect extraction pipeline\n",
    "\n",
    "llm_test_250K = (df_250K\n",
    "    .pipe(clean_text)\n",
    "    .pipe(apply_aspect_extraction)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ee4c6-2db5-4db2-bbaa-64d554dd64d0",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1fbdeaa-a110-4c61-a613-82572e1e4b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating binary aspect-sentiment columns...\n",
      "  Created 15 binary label columns\n",
      "\n",
      "Label distribution:\n",
      "quality_positive        60713.0\n",
      "service_positive        60687.0\n",
      "wait_time_positive      33851.0\n",
      "value_positive          25628.0\n",
      "cleanliness_positive    17066.0\n",
      "service_negative         9228.0\n",
      "wait_time_negative       7047.0\n",
      "quality_negative         6984.0\n",
      "quality_neutral          6461.0\n",
      "service_neutral          5300.0\n",
      "wait_time_neutral        4506.0\n",
      "value_negative           4317.0\n",
      "value_neutral            3771.0\n",
      "cleanliness_negative     3158.0\n",
      "cleanliness_neutral      2461.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Function to convert labels to columns\n",
    "def create_binary_label_columns(df):\n",
    "    \"\"\"Convert aspect_labels dict to binary columns.\"\"\"\n",
    "    print(\"Creating binary aspect-sentiment columns...\")\n",
    "    df_binary = df.copy()\n",
    "    \n",
    "    # Initialize columns\n",
    "    for aspect in aspects:\n",
    "        for sentiment in sentiments:\n",
    "            df_binary[f'{aspect}_{sentiment}'] = 0\n",
    "    \n",
    "    # Fill based on aspect_labels\n",
    "    for idx, row in df_binary.iterrows():\n",
    "        for aspect, sentiment in row['aspect_labels'].items():\n",
    "            df_binary.at[idx, f'{aspect}_{sentiment}'] = 1\n",
    "    \n",
    "    # Get label column names\n",
    "    label_cols = [f'{aspect}_{sentiment}' for aspect in aspects for sentiment in sentiments]\n",
    "    \n",
    "    # Convert to float\n",
    "    for col in label_cols:\n",
    "        df_binary[col] = df_binary[col].astype(float)\n",
    "    \n",
    "    print(f\"  Created {len(label_cols)} binary label columns\\n\")\n",
    "    return df_binary, label_cols\n",
    "\n",
    "# Create binary columns\n",
    "llm_test_250K, label_cols = create_binary_label_columns(llm_test_250K)\n",
    "\n",
    "# Summary\n",
    "print(\"Label distribution:\")\n",
    "print(llm_test_250K[label_cols].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252c5fb2-b91c-42f0-bf07-ae7affbd11b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/val split (80/20)...\n",
      "  Train size: 200,000\n",
      "  Val size:   50,000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "\n",
    "def create_train_val_split(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"Split data into train and validation sets.\"\"\"\n",
    "    print(f\"Creating train/val split ({int((1-test_size)*100)}/{int(test_size*100)})...\")\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    print(f\"  Train size: {len(train_df):,}\")\n",
    "    print(f\"  Val size:   {len(val_df):,}\\n\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "# Split\n",
    "train_df, val_df = create_train_val_split(llm_test_250K, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b9312e1-e03e-4b05-9fc5-7de7861406b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HuggingFace datasets...\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256f163d35374b099945cf86bc1eefb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ada488939445819691e649474e7533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 200,000 samples\n",
      "Val dataset:   50,000 samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text and prepare labels.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Prepare multi-label targets\n",
    "    labels = []\n",
    "    for i in range(len(examples['text'])):\n",
    "        label_row = [float(examples[col][i]) for col in label_cols]\n",
    "        labels.append(label_row)\n",
    "    \n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating HuggingFace datasets...\")\n",
    "train_dataset = Dataset.from_pandas(train_df[['text'] + label_cols])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text'] + label_cols])\n",
    "\n",
    "# Tokenize\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'] + label_cols)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['text'] + label_cols)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"Val dataset:   {len(val_dataset):,} samples\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a33ddb-7e0d-4fa3-ad54-4d7e126ff4e1",
   "metadata": {},
   "source": [
    "*Note: 0% is a display bug in the notebook/library version. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5a5d7-4ca9-420b-8ffa-85605d24ff74",
   "metadata": {},
   "source": [
    "### Train Model - DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e80a385-e898-4215-9121-41422edc7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute evaluation metrics for multi-label classification.\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = (pred.predictions > 0.5).astype(int)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9bf6aa3-708e-41ae-a516-824567520a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DistilBERT model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0fb1bfe9ac41d98299f0b7e05a0887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "classifier.bias         | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 15 output labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "print(\"Loading DistilBERT model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label_cols),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "print(f\"Model loaded with {len(label_cols)} output labels\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a9ee54c-ccf2-4319-9de4-aace89dfec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macmini/.venvs/base/lib/python3.14/site-packages/torch/utils/data/dataloader.py:1118: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 6:54:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.041512</td>\n",
       "      <td>0.035645</td>\n",
       "      <td>0.901600</td>\n",
       "      <td>0.850103</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.839742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.029207</td>\n",
       "      <td>0.034038</td>\n",
       "      <td>0.912180</td>\n",
       "      <td>0.875732</td>\n",
       "      <td>0.914469</td>\n",
       "      <td>0.857032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.021521</td>\n",
       "      <td>0.037442</td>\n",
       "      <td>0.921480</td>\n",
       "      <td>0.886648</td>\n",
       "      <td>0.905401</td>\n",
       "      <td>0.874661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad2885b9d3a472fa9e01f55affeaa0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macmini/.venvs/base/lib/python3.14/site-packages/torch/utils/data/dataloader.py:1118: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1b43db58d4454ea250aca764a4e71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macmini/.venvs/base/lib/python3.14/site-packages/torch/utils/data/dataloader.py:1118: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f31a8ee40af4beea20406680313c5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.beta', 'distilbert.embeddings.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18750, training_loss=0.033914218928019206, metrics={'train_runtime': 24849.9525, 'train_samples_per_second': 24.145, 'train_steps_per_second': 0.755, 'total_flos': 7.9498865664e+16, 'train_loss': 0.033914218928019206, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_absa',           # Default: None | (required parameter)\n",
    "    eval_strategy=\"epoch\",                 # Default: \"no\" | evaluate each epoch\n",
    "    save_strategy=\"epoch\",                 # Default: \"steps\" | save each epoch\n",
    "    load_best_model_at_end=True,           # Default: False | load best model\n",
    "    metric_for_best_model='f1',            # Default: \"loss\" | F1 score\n",
    "    logging_steps=100,                     # Default: 500 | log more frequently\n",
    "    per_device_train_batch_size=32,        # Default: 8 | 4x larger batches *(for local training, likely need to reduce for datahub)\n",
    "    dataloader_num_workers=4,              # Default: 0 | parallel data loading *(for local training, likely need to reduce for datahub)\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\\n\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b286c-09fa-4f31-8bc3-05150dedfc31",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6595050a-3e6b-42e6-aac9-1ec0a5801090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macmini/.venvs/base/lib/python3.14/site-packages/torch/utils/data/dataloader.py:1118: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 09:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Validation Results:\n",
      "  Accuracy:  0.9215\n",
      "  F1 Score:  0.8866\n",
      "  Precision: 0.9054\n",
      "  Recall:    0.8747\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nFinal Validation Results:\")\n",
    "print(f\"  Accuracy:  {results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1 Score:  {results['eval_f1']:.4f}\")\n",
    "print(f\"  Precision: {results['eval_precision']:.4f}\")\n",
    "print(f\"  Recall:    {results['eval_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d37318f-13e1-41d8-816a-3ff4721ca846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
