{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5efdae75-88b4-4ff9-805b-a104a694ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e56ff179-872c-4c86-8136-d21ea1a9d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "paths = {\n",
    "    'cleaned_for_bert': \"../data/cleaned_for_bert.csv\",\n",
    "    'absa_training_set': \"../data/absa_training_set.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6bbf7-ff58-426c-aa2f-397046e0bdc4",
   "metadata": {},
   "source": [
    "# Separate training data for manual ABSA labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d195b5fd-3ffa-49fa-9d28-a912fbd3cef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pass 1] chunks=10 sizes={1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000} unique_gmap_ids={1: 1845, 2: 1750, 3: 1675, 4: 1703, 5: 1825}\n",
      "[Pass 1] chunks=20 sizes={1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000} unique_gmap_ids={1: 1923, 2: 1893, 3: 1898, 4: 1903, 5: 1923}\n",
      "[Pass 1] chunks=30 sizes={1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000} unique_gmap_ids={1: 1944, 2: 1907, 3: 1919, 4: 1924, 5: 1935}\n",
      "[Pass 1] chunks=40 sizes={1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000} unique_gmap_ids={1: 1957, 2: 1917, 3: 1918, 4: 1926, 5: 1931}\n",
      "[Pass 1] chunks=50 sizes={1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000} unique_gmap_ids={1: 1955, 2: 1931, 3: 1937, 4: 1948, 5: 1934}\n",
      "[Pass 1] chunks=60 sizes={1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000} unique_gmap_ids={1: 1950, 2: 1932, 3: 1937, 4: 1962, 5: 1946}\n",
      "[Pass 1] chunks=70 sizes={1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000} unique_gmap_ids={1: 1952, 2: 1940, 3: 1937, 4: 1963, 5: 1958}\n",
      "[Pass 1] chunks=80 sizes={1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000} unique_gmap_ids={1: 1958, 2: 1948, 3: 1943, 4: 1967, 5: 1960}\n",
      "[Pass 1] chunks=90 sizes={1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000} unique_gmap_ids={1: 1960, 2: 1953, 3: 1952, 4: 1971, 5: 1966}\n",
      "[Pass 1] chunks=100 sizes={1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000} unique_gmap_ids={1: 1963, 2: 1950, 3: 1942, 4: 1970, 5: 1969}\n",
      "Selected unique review_ids: 10000\n",
      "rating 1: 2000 ids; unique gmap_ids in sample: 1963\n",
      "rating 2: 2000 ids; unique gmap_ids in sample: 1950\n",
      "rating 3: 2000 ids; unique gmap_ids in sample: 1941\n",
      "rating 4: 2000 ids; unique gmap_ids in sample: 1968\n",
      "rating 5: 2000 ids; unique gmap_ids in sample: 1969\n",
      "[Pass 2] chunks=10 rows_written=2498\n",
      "[Pass 2] chunks=20 rows_written=4950\n",
      "[Pass 2] chunks=30 rows_written=7585\n",
      "[Pass 2] chunks=40 rows_written=10204\n",
      "[Pass 2] chunks=50 rows_written=12589\n",
      "[Pass 2] chunks=60 rows_written=15047\n",
      "[Pass 2] chunks=70 rows_written=17526\n",
      "[Pass 2] chunks=80 rows_written=20012\n",
      "[Pass 2] chunks=90 rows_written=22707\n",
      "[Pass 2] chunks=100 rows_written=25332\n",
      "Done. Wrote: ../data/absa_training_set.csv rows: 25427\n"
     ]
    }
   ],
   "source": [
    "infile = paths[\"cleaned_for_bert\"]\n",
    "outfile = paths[\"absa_training_set\"]\n",
    "\n",
    "# Columns of interest\n",
    "useCols_full = [\"review_id\", \"rating\", \"gmap_id\", \"sentence_id\", \"sentence_text\"]\n",
    "useCols_pass1 = [\"review_id\", \"rating\", \"gmap_id\"]\n",
    "\n",
    "dtypes_pass1 = {\"review_id\": \"string\", \"rating\": \"int8\", \"gmap_id\": \"string\"}\n",
    "dtypes_full = {\n",
    "    \"review_id\": \"string\",\n",
    "    \"rating\": \"int8\",\n",
    "    \"gmap_id\": \"string\",\n",
    "    \"sentence_id\": \"int32\",\n",
    "    \"sentence_text\": \"string\",\n",
    "}\n",
    "\n",
    "# Goal: 2,000 unique reviews per rating, with a spread of gmap_ids\n",
    "num_per_rating = 2_000\n",
    "\n",
    "CHUNKSIZE = 500_000\n",
    "seed = 120\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# Try to avoid over-sampling a single business:\n",
    "# Keep at most this many sampled review_ids per gmap_id within each rating.\n",
    "# (Tune up/down depending on how diverse you want the businesses.)\n",
    "gmap_cap_per_rating = 20\n",
    "\n",
    "\n",
    "def reservoir_update_ids_capped(res_ids, res_gmap_counts, new_pairs, seen_before, k, rng, gmap_cap):\n",
    "    \"\"\"\n",
    "    Reservoir sampling over a stream of UNIQUE (review_id, gmap_id) pairs,\n",
    "    with a soft cap on how many samples can come from a single gmap_id.\n",
    "\n",
    "    res_ids: list of sampled review_ids (size <= k)\n",
    "    res_gmap_counts: dict[gmap_id] -> count currently in reservoir\n",
    "    new_pairs: iterable of (review_id, gmap_id) that are new (not seen before)\n",
    "    seen_before: number of unique ids seen so far for this rating\n",
    "    \"\"\"\n",
    "    seen = seen_before\n",
    "\n",
    "    for rid, gid in new_pairs:\n",
    "        seen += 1\n",
    "\n",
    "        # If reservoir not full, add if it doesn't violate the gmap cap\n",
    "        if len(res_ids) < k:\n",
    "            if res_gmap_counts.get(gid, 0) < gmap_cap:\n",
    "                res_ids.append(rid)\n",
    "                res_gmap_counts[gid] = res_gmap_counts.get(gid, 0) + 1\n",
    "            continue\n",
    "\n",
    "        # Reservoir full: classic reservoir sampling replacement step\n",
    "        j = rng.integers(0, seen)  # [0, seen-1]\n",
    "        if j >= k:\n",
    "            continue\n",
    "\n",
    "        # Proposed replacement index in reservoir\n",
    "        old_rid = res_ids[j]\n",
    "\n",
    "        # NOTE: we don't know old_rid's gmap_id unless we track it.\n",
    "        # We'll track rid->gid in a dict outside (rid_to_gmap) so we can update counts safely.\n",
    "        old_gid = rid_to_gmap.get(old_rid)\n",
    "\n",
    "        # Only replace if the incoming gid won't exceed cap (or it replaces same gid)\n",
    "        incoming_ok = (res_gmap_counts.get(gid, 0) < gmap_cap) or (old_gid == gid)\n",
    "        if not incoming_ok:\n",
    "            continue\n",
    "\n",
    "        # Apply replacement\n",
    "        res_ids[j] = rid\n",
    "\n",
    "        # Update gmap counts bookkeeping\n",
    "        if old_gid is not None:\n",
    "            res_gmap_counts[old_gid] = max(0, res_gmap_counts.get(old_gid, 0) - 1)\n",
    "            if res_gmap_counts[old_gid] == 0:\n",
    "                del res_gmap_counts[old_gid]\n",
    "\n",
    "        res_gmap_counts[gid] = res_gmap_counts.get(gid, 0) + 1\n",
    "\n",
    "    return res_ids, res_gmap_counts, seen\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# PASS 1: sample review_ids (unique) with gmap spread\n",
    "# -------------------------\n",
    "sample_ids = {r: [] for r in range(1, 6)}          # reservoirs (lists) of ids\n",
    "seen_unique = {r: 0 for r in range(1, 6)}          # how many unique ids seen per rating\n",
    "gmap_counts = {r: {} for r in range(1, 6)}         # per-rating gmap_id counts in reservoir\n",
    "\n",
    "# Track review_id -> gmap_id for anything currently in reservoirs (for count updates)\n",
    "rid_to_gmap = {}\n",
    "\n",
    "# global \"seen\" set so we only consider each review_id once total\n",
    "# (prevents duplicates across chunks)\n",
    "global_seen_review_ids = set()\n",
    "\n",
    "reader1 = pd.read_csv(\n",
    "    infile,\n",
    "    usecols=useCols_pass1,\n",
    "    dtype=dtypes_pass1,\n",
    "    chunksize=CHUNKSIZE,\n",
    ")\n",
    "\n",
    "for chunk_idx, chunk in enumerate(reader1, start=1):\n",
    "\n",
    "    # Defensive: keep only expected ratings\n",
    "    chunk = chunk[chunk[\"rating\"].between(1, 5)]\n",
    "\n",
    "    # For pass 1, need each review_id once.\n",
    "    # Drop within-chunk duplicates first to shrink work\n",
    "    chunk = chunk.drop_duplicates(subset=[\"review_id\"])\n",
    "\n",
    "    # Remove review_ids already processed in previous chunks\n",
    "    # (so each review_id enters the sampler exactly once)\n",
    "    mask_new = ~chunk[\"review_id\"].isin(global_seen_review_ids)\n",
    "    new_chunk = chunk.loc[mask_new]\n",
    "\n",
    "    # Add to global set\n",
    "    new_ids_all = new_chunk[\"review_id\"].astype(str).tolist()\n",
    "    global_seen_review_ids.update(new_ids_all)\n",
    "\n",
    "    # Update reservoirs per rating with those new unique ids (+ gmap_id for spreading)\n",
    "    for r in range(1, 6):\n",
    "        sub = new_chunk.loc[new_chunk[\"rating\"] == r, [\"review_id\", \"gmap_id\"]]\n",
    "\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        # Convert to python strings for stable hashing / dict keys\n",
    "        pairs = list(zip(sub[\"review_id\"].astype(str).tolist(), sub[\"gmap_id\"].astype(str).tolist()))\n",
    "\n",
    "        # Pre-register rid->gid so we can decrement counts when we replace later\n",
    "        # (This is safe; the dict will end up containing lots of rids over time,\n",
    "        # but it is much smaller than storing all review_ids in a set.)\n",
    "        for rid, gid in pairs:\n",
    "            if rid not in rid_to_gmap:\n",
    "                rid_to_gmap[rid] = gid\n",
    "\n",
    "        sample_ids[r], gmap_counts[r], seen_unique[r] = reservoir_update_ids_capped(\n",
    "            sample_ids[r],\n",
    "            gmap_counts[r],\n",
    "            pairs,\n",
    "            seen_unique[r],\n",
    "            num_per_rating,\n",
    "            rng,\n",
    "            gmap_cap_per_rating,\n",
    "        )\n",
    "\n",
    "    if chunk_idx % 10 == 0:\n",
    "        sizes = {r: len(sample_ids[r]) for r in range(1, 6)}\n",
    "        uniq_gmaps = {r: len(gmap_counts[r]) for r in range(1, 6)}\n",
    "        print(f\"[Pass 1] chunks={chunk_idx} sizes={sizes} unique_gmap_ids={uniq_gmaps}\")\n",
    "\n",
    "selected_ids = set().union(*[set(sample_ids[r]) for r in range(1, 6)])\n",
    "print(\"Selected unique review_ids:\", len(selected_ids))\n",
    "\n",
    "# Check if got 2k of each rating\n",
    "for r in range(1, 6):\n",
    "    print(f\"rating {r}: {len(sample_ids[r])} ids; unique gmap_ids in sample: {len(gmap_counts[r])}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# PASS 2: write all rows for those review_ids\n",
    "# ---------------------------------------------\n",
    "# Stream again and append matching rows to disk.\n",
    "# Avoids ever holding all selected sentences in memory.\n",
    "\n",
    "reader2 = pd.read_csv(\n",
    "    infile,\n",
    "    usecols=useCols_full,\n",
    "    dtype=dtypes_full,\n",
    "    chunksize=CHUNKSIZE,\n",
    ")\n",
    "\n",
    "# Write header once, then append\n",
    "first_write = True\n",
    "rows_written = 0\n",
    "\n",
    "for chunk_idx, chunk in enumerate(reader2, start=1):\n",
    "\n",
    "    # filter to selected review_ids\n",
    "    m = chunk[\"review_id\"].astype(str).isin(selected_ids)\n",
    "    out = chunk.loc[m]\n",
    "\n",
    "    if not out.empty:\n",
    "        out.to_csv(outfile, mode=\"w\" if first_write else \"a\", index=False, header=first_write)\n",
    "        first_write = False\n",
    "        rows_written += len(out)\n",
    "\n",
    "    if chunk_idx % 10 == 0:\n",
    "        print(f\"[Pass 2] chunks={chunk_idx} rows_written={rows_written}\")\n",
    "\n",
    "print(\"Done. Wrote:\", outfile, \"rows:\", rows_written)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75fbe85-1fee-4447-8cc7-002e7842b80b",
   "metadata": {},
   "source": [
    "# Widget for Manual ABSA Data Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d73ce37-20e6-4ef9-91e8-6299b0f0fd12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf96d17aa2c409db694440aa5cfcef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"\\n    <div style='padding:10px; border:2px solid #333; border-radius:8px; backgrounâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from datetime import datetime\n",
    "\n",
    "# =====================================================\n",
    "# CONFIG\n",
    "# =====================================================\n",
    "\n",
    "sent_file = paths[\"absa_training_set\"]\n",
    "labels_file = \"../data/absa_labels_long.csv\"\n",
    "progress_file = \"../data/absa_labeling_progress.txt\"\n",
    "\n",
    "ASPECTS = [\n",
    "    \"food_quality\",\n",
    "    \"service\",\n",
    "    \"wait_time\",\n",
    "    \"price_value\",\n",
    "    \"cleanliness\",\n",
    "    \"atmosphere\",\n",
    "    \"general\",\n",
    "]\n",
    "\n",
    "SENTIMENTS = [\"positive\", \"negative\"]\n",
    "\n",
    "# =====================================================\n",
    "# LOAD DATA\n",
    "# =====================================================\n",
    "\n",
    "df = pd.read_csv(\n",
    "    sent_file,\n",
    "    usecols=[\"review_id\", \"rating\", \"gmap_id\", \"sentence_id\", \"sentence_text\"]\n",
    ")\n",
    "\n",
    "df[\"review_id\"] = df[\"review_id\"].astype(str)\n",
    "df[\"gmap_id\"] = df[\"gmap_id\"].astype(str)\n",
    "df[\"sentence_text\"] = df[\"sentence_text\"].astype(str)\n",
    "df[\"key\"] = df[\"review_id\"] + \"::\" + df[\"sentence_id\"].astype(str)\n",
    "\n",
    "if os.path.exists(labels_file):\n",
    "    labels = pd.read_csv(labels_file)\n",
    "else:\n",
    "    labels = pd.DataFrame(columns=[\"review_id\", \"sentence_id\", \"aspect\", \"sentiment\"])\n",
    "\n",
    "labeled_keys = set(labels[\"review_id\"].astype(str) + \"::\" +\n",
    "                   labels[\"sentence_id\"].astype(str))\n",
    "\n",
    "start_idx = 0\n",
    "if os.path.exists(progress_file):\n",
    "    try:\n",
    "        start_idx = int(open(progress_file).read().strip())\n",
    "    except:\n",
    "        start_idx = 0\n",
    "\n",
    "def find_next_unlabeled(i):\n",
    "    while i < len(df) and df.iloc[i][\"key\"] in labeled_keys:\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "idx = find_next_unlabeled(start_idx)\n",
    "pending_pairs = []\n",
    "history = []\n",
    "\n",
    "# =====================================================\n",
    "# WIDGETS\n",
    "# =====================================================\n",
    "\n",
    "header = widgets.HTML(\"\")\n",
    "status = widgets.HTML(\"\")\n",
    "meta_area = widgets.HTML(\"\")\n",
    "text_area = widgets.HTML(\"\")\n",
    "pairs_box = widgets.HTML(\"\")\n",
    "msg = widgets.HTML(\"\")\n",
    "\n",
    "aspect_dd = widgets.Dropdown(\n",
    "    options=[\"(choose)\"] + ASPECTS,\n",
    "    value=\"(choose)\",\n",
    "    description=\"Aspect:\"\n",
    ")\n",
    "\n",
    "sent_dd = widgets.ToggleButtons(\n",
    "    options=SENTIMENTS,\n",
    "    description=\"Sentiment:\"\n",
    ")\n",
    "\n",
    "add_btn = widgets.Button(description=\"Add Pair\", button_style=\"info\")\n",
    "clear_btn = widgets.Button(description=\"Clear Pairs\")\n",
    "skip_btn = widgets.Button(description=\"Skip\", button_style=\"warning\")\n",
    "save_btn = widgets.Button(description=\"Save & Next\", button_style=\"success\")\n",
    "undo_btn = widgets.Button(description=\"Undo\", button_style=\"danger\")\n",
    "\n",
    "controls = widgets.HBox([add_btn, clear_btn, skip_btn, save_btn, undo_btn])\n",
    "selectors = widgets.HBox([aspect_dd, sent_dd])\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    header,\n",
    "    status,\n",
    "    meta_area,\n",
    "    text_area,\n",
    "    selectors,\n",
    "    controls,\n",
    "    pairs_box,\n",
    "    msg\n",
    "])\n",
    "\n",
    "# =====================================================\n",
    "# HEADER METRICS\n",
    "# =====================================================\n",
    "\n",
    "def update_header():\n",
    "    global labels\n",
    "\n",
    "    total_sentences = len(df)\n",
    "    labeled_sentences = len(labeled_keys)\n",
    "    percent = (labeled_sentences / total_sentences) * 100\n",
    "\n",
    "    total_pairs = len(labels)\n",
    "\n",
    "    aspect_counts = labels[\"aspect\"].value_counts().to_dict()\n",
    "    sentiment_counts = labels[\"sentiment\"].value_counts().to_dict()\n",
    "\n",
    "    aspect_html = \" | \".join(\n",
    "        [f\"{a}: {aspect_counts.get(a,0)}\" for a in ASPECTS]\n",
    "    )\n",
    "\n",
    "    sentiment_html = \" | \".join(\n",
    "        [f\"{s}: {sentiment_counts.get(s,0)}\" for s in SENTIMENTS]\n",
    "    )\n",
    "\n",
    "    last_saved = (\n",
    "        datetime.fromtimestamp(os.path.getmtime(labels_file)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        if os.path.exists(labels_file)\n",
    "        else \"N/A\"\n",
    "    )\n",
    "\n",
    "    header.value = f\"\"\"\n",
    "    <div style='padding:10px; border:2px solid #333; border-radius:8px; background:#f5f5f5'>\n",
    "        <b>Progress:</b> {labeled_sentences:,} / {total_sentences:,} sentences ({percent:.2f}%)<br>\n",
    "        <b>Total Aspect-Sentiment Pairs:</b> {total_pairs:,}<br>\n",
    "        <b>Aspect Counts:</b> {aspect_html}<br>\n",
    "        <b>Sentiment Counts:</b> {sentiment_html}<br>\n",
    "        <b>Last Saved:</b> {last_saved}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "# =====================================================\n",
    "# CORE FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def render():\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if idx >= len(df):\n",
    "        display(widgets.HTML(\"<h3>Done! ðŸŽ‰</h3>\"))\n",
    "        return\n",
    "\n",
    "    update_header()\n",
    "\n",
    "    row = df.iloc[idx]\n",
    "\n",
    "    status.value = f\"<b>Row:</b> {idx+1:,} / {len(df):,}\"\n",
    "\n",
    "    meta_area.value = (\n",
    "        f\"<b>Rating:</b> {row['rating']} &nbsp;&nbsp; \"\n",
    "        f\"<b>Review ID:</b> {row['review_id']} &nbsp;&nbsp; \"\n",
    "        f\"<b>Sentence ID:</b> {row['sentence_id']} &nbsp;&nbsp; \"\n",
    "        f\"<b>Business:</b> {row['gmap_id']}\"\n",
    "    )\n",
    "\n",
    "    safe_text = (\n",
    "        row[\"sentence_text\"]\n",
    "        .replace(\"&\", \"&amp;\")\n",
    "        .replace(\"<\", \"&lt;\")\n",
    "        .replace(\">\", \"&gt;\")\n",
    "    )\n",
    "\n",
    "    text_area.value = f\"\"\"\n",
    "    <div style='font-size:20px; padding:12px;\n",
    "                border:1px solid #ddd; border-radius:8px;\n",
    "                background:#fafafa;'>\n",
    "        {safe_text}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    if pending_pairs:\n",
    "        pairs_html = \"<ul>\" + \"\".join(\n",
    "            [f\"<li><b>{a}</b> â†’ {s}</li>\" for a, s in pending_pairs]\n",
    "        ) + \"</ul>\"\n",
    "    else:\n",
    "        pairs_html = \"<i>No labels added yet.</i>\"\n",
    "\n",
    "    pairs_box.value = f\"<b>Current Labels:</b>{pairs_html}\"\n",
    "    msg.value = \"\"\n",
    "\n",
    "    display(ui)\n",
    "\n",
    "def autosave():\n",
    "    with open(progress_file, \"w\") as f:\n",
    "        f.write(str(idx))\n",
    "\n",
    "def append_labels(rows_df):\n",
    "    global labels\n",
    "    file_exists = os.path.exists(labels_file)\n",
    "    rows_df.to_csv(labels_file, mode=\"a\", index=False,\n",
    "                   header=not file_exists)\n",
    "    labels = pd.concat([labels, rows_df], ignore_index=True)\n",
    "\n",
    "def on_add(_):\n",
    "    if aspect_dd.value == \"(choose)\":\n",
    "        msg.value = \"<b style='color:red'>Select an aspect first.</b>\"\n",
    "        return\n",
    "    pending_pairs.append((aspect_dd.value, sent_dd.value))\n",
    "    aspect_dd.value = \"(choose)\"\n",
    "    render()\n",
    "\n",
    "def on_clear(_):\n",
    "    pending_pairs.clear()\n",
    "    render()\n",
    "\n",
    "def on_skip(_):\n",
    "    global idx\n",
    "    history.append({\"idx\": idx, \"rows\": None})\n",
    "    labeled_keys.add(df.iloc[idx][\"key\"])\n",
    "    pending_pairs.clear()\n",
    "    idx = find_next_unlabeled(idx + 1)\n",
    "    autosave()\n",
    "    render()\n",
    "\n",
    "def on_save(_):\n",
    "    global idx\n",
    "\n",
    "    row = df.iloc[idx]\n",
    "\n",
    "    if not pending_pairs:\n",
    "        on_skip(None)\n",
    "        return\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"review_id\": [row[\"review_id\"]] * len(pending_pairs),\n",
    "        \"sentence_id\": [row[\"sentence_id\"]] * len(pending_pairs),\n",
    "        \"aspect\": [a for a, _ in pending_pairs],\n",
    "        \"sentiment\": [s for _, s in pending_pairs],\n",
    "    })\n",
    "\n",
    "    append_labels(out)\n",
    "\n",
    "    history.append({\"idx\": idx, \"rows\": out})\n",
    "    labeled_keys.add(row[\"key\"])\n",
    "    pending_pairs.clear()\n",
    "\n",
    "    idx = find_next_unlabeled(idx + 1)\n",
    "    autosave()\n",
    "    render()\n",
    "\n",
    "def on_undo(_):\n",
    "    global idx, labels\n",
    "\n",
    "    if not history:\n",
    "        msg.value = \"<b style='color:red'>Nothing to undo.</b>\"\n",
    "        render()\n",
    "        return\n",
    "\n",
    "    last = history.pop()\n",
    "    idx = last[\"idx\"]\n",
    "    key = df.iloc[idx][\"key\"]\n",
    "\n",
    "    if key in labeled_keys:\n",
    "        labeled_keys.remove(key)\n",
    "\n",
    "    if last[\"rows\"] is not None and os.path.exists(labels_file):\n",
    "        lab = pd.read_csv(labels_file)\n",
    "        rid = df.iloc[idx][\"review_id\"]\n",
    "        sid = df.iloc[idx][\"sentence_id\"]\n",
    "        lab = lab[~((lab[\"review_id\"] == rid) &\n",
    "                    (lab[\"sentence_id\"] == sid))]\n",
    "        lab.to_csv(labels_file, index=False)\n",
    "        labels = lab\n",
    "\n",
    "    pending_pairs.clear()\n",
    "    autosave()\n",
    "    render()\n",
    "\n",
    "add_btn.on_click(on_add)\n",
    "clear_btn.on_click(on_clear)\n",
    "skip_btn.on_click(on_skip)\n",
    "save_btn.on_click(on_save)\n",
    "undo_btn.on_click(on_undo)\n",
    "\n",
    "render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51413f-ba20-4cb0-b612-db3cd838d9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23cc55-9c5f-47c8-9457-eae8d6945cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
