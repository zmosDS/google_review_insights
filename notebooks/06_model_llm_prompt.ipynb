{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79892d75-5c66-451a-ab33-554c87ddbdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec99362-cc81-4d2f-a14c-5c60eb28aabd",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108babdc-9921-4000-bf94-7d821c5d1ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22,624,379 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>gmap_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>456e420929727f933dbaed63eff45cde53c7b92438cf0d...</td>\n",
       "      <td>1.067134e+20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Easy process, extremely friendly, helpful staf...</td>\n",
       "      <td>0x80960c29f2e3bf29:0x4b291f0d275a5699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ea2ad448a8b443c1c42c5d4ca9dd84d02fe9f2f110b993...</td>\n",
       "      <td>1.024963e+20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>My girlfriends and I took a weekend ski trip t...</td>\n",
       "      <td>0x80960c29f2e3bf29:0x4b291f0d275a5699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77efbe6a6f4d27512b59bb2f878b0ac8b533aa03a11fb7...</td>\n",
       "      <td>1.102407e+20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The team at Black Tie never disappoints our se...</td>\n",
       "      <td>0x80960c29f2e3bf29:0x4b291f0d275a5699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review_id       user_id  rating  \\\n",
       "0  456e420929727f933dbaed63eff45cde53c7b92438cf0d...  1.067134e+20     5.0   \n",
       "1  ea2ad448a8b443c1c42c5d4ca9dd84d02fe9f2f110b993...  1.024963e+20     5.0   \n",
       "2  77efbe6a6f4d27512b59bb2f878b0ac8b533aa03a11fb7...  1.102407e+20     5.0   \n",
       "\n",
       "                                                text  \\\n",
       "0  Easy process, extremely friendly, helpful staf...   \n",
       "1  My girlfriends and I took a weekend ski trip t...   \n",
       "2  The team at Black Tie never disappoints our se...   \n",
       "\n",
       "                                 gmap_id  \n",
       "0  0x80960c29f2e3bf29:0x4b291f0d275a5699  \n",
       "1  0x80960c29f2e3bf29:0x4b291f0d275a5699  \n",
       "2  0x80960c29f2e3bf29:0x4b291f0d275a5699  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv('../data/cleaned_for_LLM.csv') \n",
    "print(f\"Loaded {len(df):,} rows\\n\")\n",
    "df.head(3)\n",
    "\n",
    "# Filter out manual labeling set\n",
    "absa_ids = set(pd.read_csv('../data/absa_training_set.csv', usecols=['review_id'])['review_id'].astype(str))\n",
    "df = df[~df['review_id'].isin(absa_ids)].reset_index(drop=True)\n",
    "print(f\"After filtering absa set: {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1163c1e-bb6a-4f96-b555-7b281e7dddf5",
   "metadata": {},
   "source": [
    "### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a037f7d2-30fb-4b57-8fb9-e99dd87df8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 1,000 reviews...\n",
      "Sample size: 1,000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LLM inference is slow\n",
    "SAMPLE_SIZE = 1000  # Start small and increase slowly\n",
    "\n",
    "print(f\"Sampling {SAMPLE_SIZE:,} reviews...\")\n",
    "df_sample = df.sample(n=SAMPLE_SIZE, random_state=2)\n",
    "print(f\"Sample size: {len(df_sample):,}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84977d9-cf46-4474-8419-cf5398a63f88",
   "metadata": {},
   "source": [
    "# Define Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9eb019c-244e-4721-b014-5532e491d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(review_text):\n",
    "    \"\"\"Create ABSA prompt for LLM.\"\"\"\n",
    "    prompt = f\"\"\"Analyze this review and extract aspect-sentiment pairs.\n",
    "\n",
    "Review: {review_text}\n",
    "\n",
    "Identify mentions of these aspects:\n",
    "- food_quality\n",
    "- service\n",
    "- wait_time\n",
    "- price_value\n",
    "- cleanliness\n",
    "- atmosphere\n",
    "\n",
    "For each aspect mentioned, determine sentiment: positive, negative, or neutral.\n",
    "\n",
    "Respond ONLY with valid JSON in this format:\n",
    "{{\"food_quality\": \"positive\", \"service\": \"negative\"}}\n",
    "\n",
    "If an aspect is not mentioned, do not include it.\n",
    "\n",
    "JSON:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9812a-1f92-45fb-b49a-fe3c38f69f83",
   "metadata": {},
   "source": [
    "# Ollama API Call\n",
    "\n",
    "***Note:*** Must download Ollama first @ `ollama.ai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b6ebae-c238-4d7d-a54f-0d0a167e9de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama detected ✓\n"
     ]
    }
   ],
   "source": [
    "def query_ollama(prompt, model=\"mistral\"):\n",
    "    \"\"\"Query local Ollama model.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['ollama', 'run', model],\n",
    "            input=prompt,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        return result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test if Ollama is installed\n",
    "try:\n",
    "    test = subprocess.run(['ollama', '--version'], capture_output=True)\n",
    "    print(\"Ollama detected ✓\")\n",
    "except:\n",
    "    print(\"ERROR: Ollama not installed. Install from https://ollama.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d24b8-0f51-4f2d-b0ac-5f5b81d5f650",
   "metadata": {},
   "source": [
    "# Run LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dda583c-a8b9-470f-a60a-84f8256d1be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLM inference on 1,000 reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [48:42<00:00,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 1,000 predictions\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_aspects_llm(review_text):\n",
    "    \"\"\"Extract aspect-sentiment pairs using LLM.\"\"\"\n",
    "    prompt = create_prompt(review_text)\n",
    "    response = query_ollama(prompt, model=\"mistral\")\n",
    "    \n",
    "    try:\n",
    "        # Parse JSON response\n",
    "        aspects = json.loads(response)\n",
    "        return aspects\n",
    "    except:\n",
    "        # If JSON parsing fails, return empty\n",
    "        return {}\n",
    "\n",
    "# Run inference on sample\n",
    "print(f\"Running LLM inference on {len(df_sample):,} reviews...\")\n",
    "results = []\n",
    "\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "    aspects = extract_aspects_llm(row['text'])\n",
    "    results.append({\n",
    "        'review_id': row.get('review_id', idx),\n",
    "        'rating': row['rating'],\n",
    "        'text': row['text'],\n",
    "        'llm_predictions': aspects\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nCompleted {len(results_df):,} predictions\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569cf09-1de3-4053-813b-28fe4efcba0c",
   "metadata": {},
   "source": [
    "### Load Aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eec90f61-33d4-43d1-a782-40d83b89bddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      "\n",
      "food_quality:\n",
      "  positive            381\n",
      "  negative             15\n",
      "  neutral             140\n",
      "\n",
      "service:\n",
      "  positive            146\n",
      "  negative            158\n",
      "  neutral             288\n",
      "\n",
      "wait_time:\n",
      "  positive             15\n",
      "  negative             37\n",
      "  neutral             272\n",
      "\n",
      "price_value:\n",
      "  positive            105\n",
      "  negative             34\n",
      "  neutral             229\n",
      "\n",
      "cleanliness:\n",
      "  positive             90\n",
      "  negative             24\n",
      "  neutral             249\n",
      "\n",
      "atmosphere:\n",
      "  positive            312\n",
      "  negative             22\n",
      "  neutral             182\n"
     ]
    }
   ],
   "source": [
    "aspects = ['food_quality', 'service', 'wait_time', 'price_value', 'cleanliness', 'atmosphere']\n",
    "sentiments = ['positive', 'negative', 'neutral']\n",
    "label_cols = [f'{aspect}_{sentiment}' for aspect in aspects for sentiment in sentiments]\n",
    "\n",
    "# Initialize all labels to 0\n",
    "for col in label_cols:\n",
    "    results_df[col] = 0\n",
    "\n",
    "# Fill in predictions\n",
    "for idx, row in results_df.iterrows():\n",
    "    preds = row['llm_predictions']\n",
    "    for aspect, sentiment in preds.items():\n",
    "        if aspect in aspects and sentiment in sentiments:\n",
    "            results_df.at[idx, f'{aspect}_{sentiment}'] = 1\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "for aspect in aspects:\n",
    "    print(f\"\\n{aspect}:\")\n",
    "    for sentiment in sentiments:\n",
    "        col = f'{aspect}_{sentiment}'\n",
    "        print(f\"  {sentiment:<10} {results_df[col].sum():>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125d4cc-0b17-47b4-99f2-818a6dff4b6d",
   "metadata": {},
   "source": [
    "## Ground Truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f90ac3-f9fd-4d24-900f-ccb1325ed084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same weak supervision as other models\n",
    "def get_sentiment(rating):\n",
    "    if rating <= 2:\n",
    "        return 'negative'\n",
    "    elif rating >= 4:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "aspect_keywords = {\n",
    "    'food_quality': ['food', 'meal', 'dish', 'taste', 'flavor', 'delicious', 'fresh',\n",
    "                     'menu', 'order', 'coffee', 'drink', 'burger', 'chicken', 'pizza'],\n",
    "    'service': ['service', 'staff', 'employee', 'manager', 'worker', 'associate',\n",
    "                'waiter', 'waitress', 'server', 'cashier', 'friendly', 'rude',\n",
    "                'helpful', 'unprofessional', 'told', 'said', 'asked'],\n",
    "    'wait_time': ['wait', 'waiting', 'slow', 'fast', 'quick', 'minutes', 'hour',\n",
    "                  'line', 'queue', 'busy', 'long', 'forever', 'delay', 'prompt'],\n",
    "    'price_value': ['price', 'prices', 'expensive', 'cheap', 'cost', 'value', 'worth',\n",
    "                    'affordable', 'overpriced', 'reasonable', 'pricey', 'money', 'deal'],\n",
    "    'cleanliness': ['clean', 'dirty', 'filthy', 'hygiene', 'gross', 'spotless',\n",
    "                    'mess', 'messy', 'tidy', 'maintained'],\n",
    "    'atmosphere': ['atmosphere', 'ambiance', 'decor', 'seating', 'crowded', 'quiet',\n",
    "                   'noisy', 'comfortable', 'cozy', 'parking', 'location', 'space']\n",
    "}\n",
    "\n",
    "# Create ground truth labels\n",
    "results_df['sentiment'] = results_df['rating'].map(get_sentiment)\n",
    "\n",
    "for aspect, keywords in aspect_keywords.items():\n",
    "    pattern = '|'.join(keywords)\n",
    "    aspect_mentioned = results_df['text'].str.lower().str.contains(pattern, na=False)\n",
    "    for sentiment in sentiments:\n",
    "        results_df[f'{aspect}_{sentiment}_true'] = ((aspect_mentioned) & (results_df['sentiment'] == sentiment)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "933cee3a-481c-41c0-86b3-d818bb733a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM (Mistral - 1K Sample):\n",
      "\n",
      "F1 Score (macro):    0.2919\n",
      "F1 Score (weighted): 0.4819\n",
      "\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "food_quality_positive       0.65      0.87      0.75       285\n",
      "food_quality_negative       0.60      0.20      0.31        44\n",
      " food_quality_neutral       0.09      0.41      0.15        32\n",
      "     service_positive       0.74      0.43      0.54       253\n",
      "     service_negative       0.25      0.89      0.39        44\n",
      "      service_neutral       0.01      0.12      0.02        26\n",
      "   wait_time_positive       0.67      0.08      0.15       119\n",
      "   wait_time_negative       0.32      0.43      0.37        28\n",
      "    wait_time_neutral       0.01      0.21      0.03        19\n",
      " price_value_positive       0.73      0.66      0.69       117\n",
      " price_value_negative       0.18      0.46      0.26        13\n",
      "  price_value_neutral       0.01      0.11      0.02        19\n",
      " cleanliness_positive       0.48      0.88      0.62        49\n",
      " cleanliness_negative       0.25      0.55      0.34        11\n",
      "  cleanliness_neutral       0.00      0.12      0.01         8\n",
      "  atmosphere_positive       0.23      0.75      0.36        97\n",
      "  atmosphere_negative       0.18      0.33      0.24        12\n",
      "   atmosphere_neutral       0.02      0.33      0.03         9\n",
      "\n",
      "            micro avg       0.25      0.56      0.34      1185\n",
      "            macro avg       0.30      0.43      0.29      1185\n",
      "         weighted avg       0.54      0.56      0.48      1185\n",
      "          samples avg       0.32      0.40      0.32      1185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions and ground truth\n",
    "y_true = results_df[[f'{col}_true' for col in label_cols]].values\n",
    "y_pred = results_df[label_cols].values\n",
    "\n",
    "print(\"LLM (Mistral - 1K Sample):\")\n",
    "print()\n",
    "print(f\"F1 Score (macro):    {f1_score(y_true, y_pred, average='macro', zero_division=0):.4f}\")\n",
    "print(f\"F1 Score (weighted): {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "print()\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_cols, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a29de-2538-4ef2-91c9-43aeb252cde2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
