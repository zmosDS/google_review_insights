{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad90a916-375c-4b47-a32d-49585abefdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, f1_score\n",
    "import random \n",
    "import warnings\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a25d24-ee9e-497a-b78e-beaa0b3ed860",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ede892-90bd-405d-8101-a45391113f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ff907a1d6c489da90d4a7a185b41de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22,624,379 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd038ae656c94dc8b445ad1945edce8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/22624379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering absa set: 22,614,379 rows\n"
     ]
    }
   ],
   "source": [
    "# Load as HuggingFace dataset\n",
    "dataset = Dataset.from_csv('../data/cleaned_for_LLM.csv')\n",
    "print(f\"Loaded {len(dataset):,} rows\")\n",
    "\n",
    "# Filter out manual labeling set\n",
    "absa_ids = set(pd.read_csv('../data/absa_training_set.csv', usecols=['review_id'])['review_id'].astype(str))\n",
    "dataset = dataset.filter(lambda x: x['review_id'] not in absa_ids)\n",
    "print(f\"After filtering absa set: {len(dataset):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132151a2-e61e-41ac-b137-5433270a5aaa",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c355f269-e439-4e72-a742-75ae69efded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute evaluation metrics for multi-label classification.\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = (pred.predictions > 0.5).astype(int)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b6732b-a1eb-4a44-a636-0cd3a21c0613",
   "metadata": {},
   "source": [
    "## RoBERTa Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46134290-f251-4adf-b53b-517f18534586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd890366fa9487183d473708aaa1361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22614379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0dd53999fd494fad709453e9e22c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/35 shards):   0%|          | 0/22614379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "aspect_cols = [\n",
    "             'product_quality_positive', 'product_quality_negative',\n",
    "             'service_positive', 'service_negative',\n",
    "             'wait_time_positive', 'wait_time_negative',\n",
    "             'price_value_positive', 'price_value_negative',\n",
    "             'cleanliness_positive', 'cleanliness_negative',\n",
    "             'atmosphere_positive', 'atmosphere_negative',\n",
    "             'general_positive', 'general_negative'\n",
    "             ]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text and prepare multi-label targets.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        padding='longest',  # only pads longest batch, not always full 128\n",
    "        truncation=True,\n",
    "        max_length=128     \n",
    "    )\n",
    "    labels = []\n",
    "    for i in range(len(examples['text'])):\n",
    "        label_row = [float(examples[col][i]) for col in aspect_cols]\n",
    "        labels.append(label_row)\n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "tokenized.save_to_disk('../data/roberta_tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e0192-7a08-48a0-8d2e-cb7a87bb23e4",
   "metadata": {},
   "source": [
    "***Note:*** 0% is a display bug in the notebook/library version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a89408-9d03-4e01-98bb-c5ed8545e353",
   "metadata": {},
   "source": [
    "## RoBERTa-base (25k Sample)\n",
    "\n",
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3601f586-3001-4f6a-a204-3d0a85b31d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RoBERTa model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323908eced8f4771a6b6f33bdae05181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 18 output labels\n",
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2112' max='2112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2112/2112 1:47:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.060443</td>\n",
       "      <td>0.057929</td>\n",
       "      <td>0.835200</td>\n",
       "      <td>0.782257</td>\n",
       "      <td>0.814565</td>\n",
       "      <td>0.783551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.046927</td>\n",
       "      <td>0.045346</td>\n",
       "      <td>0.857200</td>\n",
       "      <td>0.836677</td>\n",
       "      <td>0.887658</td>\n",
       "      <td>0.821060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037241</td>\n",
       "      <td>0.045397</td>\n",
       "      <td>0.867200</td>\n",
       "      <td>0.844941</td>\n",
       "      <td>0.877104</td>\n",
       "      <td>0.832072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8fee1da9834d668ebacf1b8fb3590d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4820cf4bfe74a25865aff3a89468d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d0b86ea7e546c2ac5fc301f4cca697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.beta', 'roberta.embeddings.LayerNorm.gamma', 'roberta.encoder.layer.0.attention.output.LayerNorm.beta', 'roberta.encoder.layer.0.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.0.output.LayerNorm.beta', 'roberta.encoder.layer.0.output.LayerNorm.gamma', 'roberta.encoder.layer.1.attention.output.LayerNorm.beta', 'roberta.encoder.layer.1.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.1.output.LayerNorm.beta', 'roberta.encoder.layer.1.output.LayerNorm.gamma', 'roberta.encoder.layer.2.attention.output.LayerNorm.beta', 'roberta.encoder.layer.2.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.2.output.LayerNorm.beta', 'roberta.encoder.layer.2.output.LayerNorm.gamma', 'roberta.encoder.layer.3.attention.output.LayerNorm.beta', 'roberta.encoder.layer.3.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.3.output.LayerNorm.beta', 'roberta.encoder.layer.3.output.LayerNorm.gamma', 'roberta.encoder.layer.4.attention.output.LayerNorm.beta', 'roberta.encoder.layer.4.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.4.output.LayerNorm.beta', 'roberta.encoder.layer.4.output.LayerNorm.gamma', 'roberta.encoder.layer.5.attention.output.LayerNorm.beta', 'roberta.encoder.layer.5.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.5.output.LayerNorm.beta', 'roberta.encoder.layer.5.output.LayerNorm.gamma', 'roberta.encoder.layer.6.attention.output.LayerNorm.beta', 'roberta.encoder.layer.6.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.6.output.LayerNorm.beta', 'roberta.encoder.layer.6.output.LayerNorm.gamma', 'roberta.encoder.layer.7.attention.output.LayerNorm.beta', 'roberta.encoder.layer.7.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.7.output.LayerNorm.beta', 'roberta.encoder.layer.7.output.LayerNorm.gamma', 'roberta.encoder.layer.8.attention.output.LayerNorm.beta', 'roberta.encoder.layer.8.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.8.output.LayerNorm.beta', 'roberta.encoder.layer.8.output.LayerNorm.gamma', 'roberta.encoder.layer.9.attention.output.LayerNorm.beta', 'roberta.encoder.layer.9.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.9.output.LayerNorm.beta', 'roberta.encoder.layer.9.output.LayerNorm.gamma', 'roberta.encoder.layer.10.attention.output.LayerNorm.beta', 'roberta.encoder.layer.10.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.10.output.LayerNorm.beta', 'roberta.encoder.layer.10.output.LayerNorm.gamma', 'roberta.encoder.layer.11.attention.output.LayerNorm.beta', 'roberta.encoder.layer.11.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.11.output.LayerNorm.beta', 'roberta.encoder.layer.11.output.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2112, training_loss=0.06492481406100771, metrics={'train_runtime': 6456.9252, 'train_samples_per_second': 10.454, 'train_steps_per_second': 0.327, 'total_flos': 1.776254759424e+16, 'train_loss': 0.06492481406100771, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration for each run\n",
    "SAMPLE_SIZE   = 25_000\n",
    "TEST_SIZE     = 0.1\n",
    "EPOCHS        = 3\n",
    "TRAIN_BATCH   = 64\n",
    "EVAL_BATCH    = 128\n",
    "LEARNING_RATE = 2e-5\n",
    "SEED          = 2\n",
    "BALANCE_CLASSES = None\n",
    "CLASS_WEIGHTS = None\n",
    "\n",
    "# Sample & Split\n",
    "random.seed(SEED)\n",
    "sample       = tokenized_dataset.select(random.sample(range(len(tokenized_dataset)), SAMPLE_SIZE))\n",
    "split        = sample.train_test_split(test_size=TEST_SIZE, seed=SEED)\n",
    "train_subset = split['train']\n",
    "test_subset  = split['test']\n",
    "\n",
    "print(f\"Train subset: {len(train_subset):,}\")\n",
    "print(f\"Test subset:  {len(test_subset):,}\")\n",
    "\n",
    "# Load RoBERTa-base Model \n",
    "print(\"Loading RoBERTa model...\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(aspect_cols),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "print(f\"Model loaded with {len(aspect_cols)} output labels\\n\")\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results_roberta_{SAMPLE_SIZE}',\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=TRAIN_BATCH,\n",
    "    per_device_eval_batch_size=EVAL_BATCH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=test_subset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Starting training...\\n\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc478424-805a-4a9f-a5b8-039be7d11326",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496d73ca-cebc-4c2e-8e65-7750dd0b716b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa (25,000 Sample):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='392' max='352' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [352/352 09:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9095\n",
      "Test Accuracy:     0.8672\n",
      "\n",
      "F1 Score (macro):    0.5889\n",
      "F1 Score (weighted): 0.8449\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       723\n",
      "           1       0.72      0.64      0.68        80\n",
      "           2       0.46      0.14      0.21        88\n",
      "           3       0.96      0.95      0.96       607\n",
      "           4       0.84      0.84      0.84       110\n",
      "           5       0.53      0.16      0.25        50\n",
      "           6       0.92      0.90      0.91       327\n",
      "           7       0.83      0.75      0.79        76\n",
      "           8       0.50      0.10      0.17        50\n",
      "           9       0.92      0.95      0.93       260\n",
      "          10       0.86      0.62      0.72        61\n",
      "          11       0.20      0.02      0.04        44\n",
      "          12       0.96      0.94      0.95       132\n",
      "          13       0.89      0.70      0.78        23\n",
      "          14       0.00      0.00      0.00        16\n",
      "          15       0.95      0.95      0.95       213\n",
      "          16       0.75      0.29      0.41        21\n",
      "          17       0.20      0.04      0.07        25\n",
      "\n",
      "   micro avg       0.92      0.83      0.87      2906\n",
      "   macro avg       0.69      0.55      0.59      2906\n",
      "weighted avg       0.88      0.83      0.84      2906\n",
      " samples avg       0.56      0.55      0.55      2906\n",
      "\n",
      "\n",
      "\n",
      "Label Index Key:\n",
      "  0: food_quality_positive\n",
      "  1: food_quality_negative\n",
      "  2: food_quality_neutral\n",
      "  3: service_positive\n",
      "  4: service_negative\n",
      "  5: service_neutral\n",
      "  6: wait_time_positive\n",
      "  7: wait_time_negative\n",
      "  8: wait_time_neutral\n",
      "  9: price_value_positive\n",
      "  10: price_value_negative\n",
      "  11: price_value_neutral\n",
      "  12: cleanliness_positive\n",
      "  13: cleanliness_negative\n",
      "  14: cleanliness_neutral\n",
      "  15: atmosphere_positive\n",
      "  16: atmosphere_negative\n",
      "  17: atmosphere_neutral\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_subset)\n",
    "y_pred = (predictions.predictions > 0.5).astype(int)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "print(f\"\\nRoBERTa ({SAMPLE_SIZE:,} Sample) â€” Epochs: {EPOCHS} | LR: {LEARNING_RATE} | Batch: {TRAIN_BATCH}\")\n",
    "print(f\"Training Accuracy: {trainer.evaluate(train_subset)['eval_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy:     {trainer.evaluate(test_subset)['eval_accuracy']:.4f}\")\n",
    "print(f\"\\nF1 Score (macro):    {f1_score(y_true, y_pred, average='macro', zero_division=0):.4f}\")\n",
    "print(f\"F1 Score (weighted): {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=aspect_cols, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df0578-e5b5-45c7-bdee-13e083959424",
   "metadata": {},
   "source": [
    "### Clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1db9c-f9da-4d11-bb35-604a41e86f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "print(\"Memory cleared.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
